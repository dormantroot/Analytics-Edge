#######################################################
### Motor Vehicle Theft Analysis in Chicago ###########
#######################################################
mvt = read.csv("mvtWeek1.csv")
# Dimensions
dim(mvt)
# Summary
summary(mvt)
str(mvt)
# ID Maximum
max(mvt$ID)
# Beat Minimum
min(mvt$Beat)
# Number of observations with Arrest Variable=TRUE
dim(mvt[which(mvt$Arrest==TRUE),])
# Number of crimes that occurred in the Alley
dim(mvt[which(mvt$LocationDescription=='ALLEY'),])
# Date of crime
mvt$Date
# median date
DateConvert = as.Date(strptime(mvt$Date, "%m/%d/%y %H:%M"))
summary(DateConvert)
# Custom date fields
mvt$Month = months(DateConvert)
mvt$Weekday = weekdays(DateConvert)
mvt$Date = DateConvert
# Which month did the fewest motor vehicle thefts occur?
min(table(mvt$Month))
# Which week did the fewest motor vehicle thefts occur?
table(mvt$Weekday)
max(table(mvt$Weekday))
# Number of observations with Arrest Variable=TRUE
s = mvt[which(mvt$Arrest==TRUE),]
table(s$Month)
max(s$Month)
getwd()
setwd("c:/Work/Analytics-Edge/Data")
#######################################################
### Motor Vehicle Theft Analysis in Chicago ###########
#######################################################
setwd("c:/Work/Analytics-Edge/Data")
mvt = read.csv("mvtWeek1.csv")
# Dimensions
dim(mvt)
# Summary
summary(mvt)
str(mvt)
# ID Maximum
max(mvt$ID)
# Beat Minimum
min(mvt$Beat)
# Number of observations with Arrest Variable=TRUE
dim(mvt[which(mvt$Arrest==TRUE),])
# Number of crimes that occurred in the Alley
dim(mvt[which(mvt$LocationDescription=='ALLEY'),])
# Date of crime
mvt$Date
# median date
DateConvert = as.Date(strptime(mvt$Date, "%m/%d/%y %H:%M"))
summary(DateConvert)
# Custom date fields
mvt$Month = months(DateConvert)
mvt$Weekday = weekdays(DateConvert)
mvt$Date = DateConvert
# Which month did the fewest motor vehicle thefts occur?
min(table(mvt$Month))
# Which week did the fewest motor vehicle thefts occur?
table(mvt$Weekday)
max(table(mvt$Weekday))
# Number of observations with Arrest Variable=TRUE
s = mvt[which(mvt$Arrest==TRUE),]
table(s$Month)
max(s$Month)
# Histogram
hist(mvt$Date, breaks=100)
boxplot(s)
boxplot(s$Date)
boxplot(mvt$Date~mvt$Arrest,mvt)
table(mvt$Arrest, mvt$Date)
years(DateConvert)
year(DateConvert)
format(mvt$Date, "%Y")
s = mvt[which(mvt$Arrest==TRUE),]
boxplot(mvt$Date~format(mvt$Date, "%Y"),mvt)
s = mvt[which(mvt$Arrest==TRUE),]
table(mvt$Arrest, format(mvt$Date, "%Y"))
prop.table(table(mvt$Arrest, format(mvt$Date, "%Y")))
table(mvt$LocationDescription)
sort(table(mvt$LocationDescription))
head(sort(table(mvt$LocationDescription)),5)
tail(sort(table(mvt$LocationDescription)),6)
mvt[which(mvt$LocationDescription != "OTHER")]
mvt[which(mvt$LocationDescription != "OTHER"),]
table(mvt[which(mvt$LocationDescription != "OTHER"),])
table(mvt$LocationDescription)
tail(sort(table(mvt$LocationDescription)),6)
dim(mvt[which(mvt$LocationDescription == "ALLEY"),])
dim(mvt[which(mvt$LocationDescription == "ALLEY"|mvt$LocationDescription == "DRIVEWAY - RESIDENTIAL"|mvt$LocationDescription == "PARKING LOT/GARAGE(NON.RESID.)"),])
dim(mvt[which(mvt$LocationDescription == "ALLEY"|mvt$LocationDescription == "DRIVEWAY - RESIDENTIAL"|mvt$LocationDescription == "PARKING LOT/GARAGE(NON.RESID.)"|mvt$LocationDescription == "STREET")|mvt$LocationDescription == "GAS STATION",])
dim(mvt[which(mvt$LocationDescription == "ALLEY"|mvt$LocationDescription == "DRIVEWAY - RESIDENTIAL"|mvt$LocationDescription == "PARKING LOT/GARAGE(NON.RESID.)"|mvt$LocationDescription == "STREET"|mvt$LocationDescription == "GAS STATION"),])
Top5 = mvt[which(mvt$LocationDescription == "ALLEY"|mvt$LocationDescription == "DRIVEWAY - RESIDENTIAL"|mvt$LocationDescription == "PARKING LOT/GARAGE(NON.RESID.)"|mvt$LocationDescription == "STREET"|mvt$LocationDescription == "GAS STATION"),]
Top5$LocationDescription = factor(Top5$LocationDescription)
str(Top5)
table(Top5$LocationDescription)
table(Top5$Arrest, Top5$LocationDescription)
gatStationInc = mvt[mvt$LocationDescription %in% "GAS STATION",]
gasStationInc = mvt[mvt$LocationDescription %in% "GAS STATION",]
table(gasStationInc)
table(gasStationInc$Weekday)
residential = mvt[mvt$LocationDescription %in% "DRIVEWAY - RESIDENTIAL",]
table(residential$Weekday)
setwd("c:/Work/Analytics-Edge/Data")
CPS = read.csv("CPSData.csv")
str(CPS)
summary(CPS$Industry)
desc(summary(CPS$Industry))
sort(table(CPS$State))
dim(CPS[which(CPS$Citizenship=='Citizen'),])
table(CPS$Citizenship)
dim(CPS[which(CPS$Citizenship=='Native Citizen'|CPS$Citizenship=='Naturalized Citizen'),])
setwd("c:/Work/Analytics-Edge/Data")
CPS = read.csv("AnonymityPoll.csv")
str(CPS)
table(CPS$Smartphone)
table(CPS$Smartphone, na.rm=FALSE)
table(CPS$Smartphone)
setwd("c:/Work/Analytics-Edge/Data")
# Read in data
wine = read.csv("wine.csv")
str(wine)
summary(wine)
# Linear Regression (one variable)
model1 = lm(Price ~ AGST, data=wine)
summary(model1)
model1$residuals
SSE = sum(model1$residuals^2)
SSE
model3 = lm(Price ~ AGST + HarvestRain + WinterRain + Age + FrancePop, data=wine)
summary(model3)
setwd("c:/Work/Analytics-Edge/Data")
NBA = read.csv("NBA_train.csv")
str(NBA)
# How many wins to make the playoffs?
table(NBA$W, NBA$Playoffs)
# How many wins to make the playoffs?
table(NBA$W, NBA$Playoffs)
# Compute Points Difference
NBA$PTSdiff = NBA$PTS - NBA$oppPTS
# Check for linear relationship
plot(NBA$PTSdiff, NBA$W)
WinsReg = lm(W ~ PTSdiff, data=NBA)
summary(WinsReg)
# Linear regression model for wins
WinsReg = lm(W ~ PTSdiff, data=NBA)
summary(WinsReg)
# Linear regression model for points scored
PointsReg = lm(PTS ~ X2PA + X3PA + FTA + AST + ORB + DRB + TOV + STL + BLK, data=NBA)
summary(PointsReg)
PointsReg$residuals
SSE = sum(PointsReg$residuals^2)
SSE
# Root mean squared error
RMSE = sqrt(SSE/nrow(NBA))
RMSE
summary(PointsReg)
PointsReg2 = lm(PTS ~ X2PA + X3PA + FTA + AST + ORB + DRB + STL + BLK, data=NBA)
summary(PointsReg2)
PointsReg2 = lm(PTS ~ X2PA + X3PA + FTA + AST + ORB + DRB + STL + BLK, data=NBA)
summary(PointsReg2)
PointsReg3 = lm(PTS ~ X2PA + X3PA + FTA + AST + ORB + STL + BLK, data=NBA)
summary(PointsReg3)
PointsReg4 = lm(PTS ~ X2PA + X3PA + FTA + AST + ORB + STL, data=NBA)
summary(PointsReg4)
SSE_4 = sum(PointsReg4$residuals^2)
RMSE_4 = sqrt(SSE_4/nrow(NBA))
SSE_4
RMSE_4
summary(climatelm)
#First, read in the data and split it using the subset command:
climate = read.csv("climate_change.csv")
train = subset(climate, Year <= 2006)
test = subset(climate, Year > 2006)
#Then, you can create the model using the command:
climatelm = lm(Temp ~ MEI + CO2 + CH4 + N2O + CFC.11 + CFC.12 + TSI + Aerosols, data=train)
#Lastly, look at the model using summary(climatelm). The Multiple R-squared value is 0.7509.
#1.2
# If you look at the model we created in the previous problem using summary(climatelm), all of the variables have at least one star except for CH4 and N2O. So MEI, CO2, CFC.11, CFC.12, TSI, and Aerosols are all significant.
summary(climatelm)
#4
# You can create a model using the step function by typing:
#
StepModel = step(climateLM)
#
# where "climateLM" is the name of the full model.
#
# If you look at the summary of the model with summary(StepModel), you can see that the R-squared value is 0.75, and only CH4 was removed.
#4
# You can create a model using the step function by typing:
#
StepModel = step(climatelm)
#
# where "climateLM" is the name of the full model.
#
# If you look at the summary of the model with summary(StepModel), you can see that the R-squared value is 0.75, and only CH4 was removed.
StepModel = step(climatelm)
summary(StepModel)
setwd("c:/Work/Analytics-Edge/Data")
songs = read.csv("songs.csv")
str(songs)
result = songs[which(songs$year == 2010),]  # songs from 2010
str(result)
result = songs[which(songs$artistname == "Michael Jackson"),]  # songs from Michael Jackson
str(result)
result = songs[which(songs$artistname == "Michael Jackson"),]  # songs from Michael Jackson
top10 = result[which(result$Top10 == 1),]
dim(top10)
top10
top10$songtitle
top10[,"songtitle"]
str(top10)
top10$songtitle
top10[,c("songtitle", "Top10")]
str(result)
table(result$timesignature)
table(songs$timesignature)
table(songs$tempo)
result = songs[order(songs$tempo), ] # Order songs descending based on tempo
result = songs[order(songs$tempo), ] # Order songs descending based on tempo
str(result)
head(result,10)
tail(result,10)
result = songs[with(songs, order(songs$tempo,decreasing=TRUE)), ]
tail(result,10)
tail(result,10)
tail(result,10)
result = songs[with(songs, order(songs$tempo,decreasing=FALSE)), ]
tail(result,10)
head(result,10)
result = songs[with(songs, order(songs$tempo,decreasing=TRUE)), ]
head(result,10)
head(result$songtitle,10)
head(result[,c("year","tempo")],10)  #Songs with the highest tempo
head(result[,c("year","songtitle",tempo")],10)  #Songs with the highest tempo
head(result[,c("year","songtitle","tempo")],10)  #Songs with the highest tempo
head(result[,c("year","songtitle","tempo")],10)  #Songs with the highest tempo
SongsTrain = subset(songs, year <= 2009,))
SongsTrain = subset(songs, year <= 2009,)
SongsTrain = subset(songs, year <= 2009,)
SongsTest = subset(songs, year == 2010,)
str(SongsTrain)
str(SongsTrain)
str(SongsTest)
nonvars = c("year", "songtitle", "artistname", "songID", "artistID")
SongsTrain = SongsTrain[ , !(names(SongsTrain) %in% nonvars) ]   # remove the above variables from Train
SongsTest = SongsTest[ , !(names(SongsTest) %in% nonvars) ]      # remove the above variables from Test
SongsLog1 = glm(Top10 ~ ., data=SongsTrain, family=binomial)     # build a logistic regression model to predict Top10
SongsTop10 = glm(Top10 ~ ., data=SongsTrain, family=binomial)     # build a logistic regression model to predict Top10
summary(SongsTop10)
cor(songs$loudness, songs$energy)    # find correlation between loudness and energy
cor(songs$loudness, songs$energy)    # loudness and energy are highly correlated
Model2 = glm(Top10 ~ . -loudness, data=SongsTrain, family=binomial)
summary(Model2)
SongsLog2 = glm(Top10 ~ . -loudness, data=SongsTrain, family=binomial)
summary(SongsLog2)
Model3 = glm(Top10 ~ . -energy, data=SongsTrain, family=binomial)
summary(Model3)
predictTest = predict(Model3, newdata=SongsTest, type="response")
confMat = table(SongsTest$Top10, predictTest >= 0.45)
confMat
(confMat[1,1] + confMat[2,2])/sum(confMat)
baseline = table(SongsTest$Top10)
baseline[1]/(baseline[1]+baseline[2])
baseline
predictTest = predict(Model3, newdata=SongsTest, type="response")
# Confusion Matrix with threshold of 0.45
confMat = table(SongsTest$Top10, predictTest >= 0.45)
confMat
confMat[2,2]/as.numeric(rowSums(confMat)[2])
confMat[1,1]/as.numeric(rowSums(confMat)[1])
source('~/.active-rstudio-document', echo=TRUE)
summary(parole)
str(parole)
summary(parole)
parole$state = as.factor(parole$state)
parole$crime = as.factor(parole$crime)
summary(parole)
set.seed(144)
library(caTools)
split = sample.split(parole$violator, SplitRatio = 0.7)
train = subset(parole, split == TRUE)
test = subset(parole, split == FALSE)
dim(parole)
dim(train)
dim(test)
mod = glm(violator ~ ., data=train, family="binomial")
summary(mod)
# Significant predictors
id = which(summary(mod)$coeff[,4] < 0.05)
id
# Cofficients of the significant predictors
coeff.sig <- summary(mod)$coeff[,1][id]
coeff.sig
odds = as.numeric(exp(coefficients(mod)[c("(Intercept)")]+coefficients(mod)[c("male")]+coefficients(mod)[c("race")] + coefficients(mod)[c("time.served")]*3 + coefficients(mod)[c("max.sentence")]*12 + coefficients(mod)[c("multiple.offenses")]*0 + coefficients(mod)[c("crime2")]))
p = odds/(1+odds)
odds
p
testPred = predict(mod, newdata=test, type="response")
testPred
max(testPred)
table(test$violator, testPred > 0.5)
12/23; 167/179; 179/202
table(test$violator)
179/202
library(ROCR)
ROCRpred = prediction(testPred, test$violator)
install.packages("ROCR")
library("ROCR", lib.loc="~/R/win-library/3.1")
library(ROCR)
ROCRpred = prediction(testPred, test$violator)
as.numeric(performance(ROCRpred, "auc")@y.values)
setwd("c:/Work/Analytics-Edge/Data")
loans = read.csv("loans.csv")
str(loans)
summary(loans)
# Variables with at least one missing observation
names(which(sapply(loans, function(x)sum(is.na(x))>=1)=="TRUE"))
# Proportion of not fully paid loans
as.numeric(table(loans$not.fully.paid)/nrow(loans))[2]
# Data frame with missing observations
missing = subset(loans, is.na(log.annual.inc) | is.na(days.with.cr.line) | is.na(revol.util) | is.na(inq.last.6mths) | is.na(delinq.2yrs) | is.na(pub.rec))
nrow(missing)  # removing this small number of observations would not lead to overfitting
# Proportion of not fully paid in the missing data frame
as.numeric(table(missing$not.fully.paid)/nrow(missing))[2] # This rate is similar to the 16.01% across all loans, so the form of biasing described is not an issue
# Load VIM library
library(VIM)
set.seed(144)
# Set variables to be imputed. All the predictors
#vars.for.imputation = setdiff(names(loans), "not.fully.paid")
#imputed = irmi(loans[vars.for.imputation])
#loans[vars.for.imputation] = imputed
#summary(loans)
# Read imputed data set to compare to the above imputed data
loans_imputed = read.csv("loans_imputed.csv")  # Use this instead of the one developed using VIM
summary(loans_imputed)
setwd("c:/Work/Analytics-Edge/Data")
loans = read.csv("loans.csv")
str(loans)
summary(loans)
# Variables with at least one missing observation
names(which(sapply(loans, function(x)sum(is.na(x))>=1)=="TRUE"))
# Proportion of not fully paid loans
as.numeric(table(loans$not.fully.paid)/nrow(loans))[2]
# Data frame with missing observations
missing = subset(loans, is.na(log.annual.inc) | is.na(days.with.cr.line) | is.na(revol.util) | is.na(inq.last.6mths) | is.na(delinq.2yrs) | is.na(pub.rec))
nrow(missing)  # removing this small number of observations would not lead to overfitting
# Proportion of not fully paid in the missing data frame
as.numeric(table(missing$not.fully.paid)/nrow(missing))[2] # This rate is similar to the 16.01% across all loans, so the form of biasing described is not an issue
# Load VIM library
library(VIM)
set.seed(144)
# Set variables to be imputed. All the predictors
#vars.for.imputation = setdiff(names(loans), "not.fully.paid")
#imputed = irmi(loans[vars.for.imputation])
#loans[vars.for.imputation] = imputed
#summary(loans)
# Read imputed data set to compare to the above imputed data
loans_imputed = read.csv("loans_imputed.csv")  # Use this instead of the one developed using VIM
summary(loans_imputed)
str(loans)
summary(loans)
names(which(sapply(loans, function(x)sum(is.na(x))>=1)=="TRUE"))
loans_imputed$purpose = as.factor(loans_imputed$purpose)
set.seed(144)
# Load caTools to use sample.split function
library(caTools)
split = sample.split(loans_imputed$not.fully.paid, SplitRatio = 0.7)
train = subset(loans_imputed, split == TRUE)
test = subset(loans_imputed, split == FALSE)
# Build the model using training set
mod = glm(not.fully.paid ~ ., data=train, family="binomial")
# index of Significant predictors
id = which(summary(mod)$coeff[,4] < 0.05)
coeff.sig = summary(mod)$coeff[,1][id]
names(coeff.sig)
Logit(700)
