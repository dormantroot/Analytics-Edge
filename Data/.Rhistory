---
library(ggplot2)
library(caret)
setwd("C:/Work/Machine-Learning-Experiments/Practical Data Science/Data")
load("psub.RData")
str(psub)
head(psub,2)
hist(psub$PINCP, main="Distribution of Annual Salary", xlab="Annual Salary")
dtest$predLogPINCP
source.with.encoding('~/.active-rstudio-document', encoding='UTF-8', echo=TRUE)
dtest$predLogPINCP
dtest$PINCP
log(dtest$PINCP)
dtest$PINCP
tail(dtest$predLogPINCP,`)
tail(dtest$predLogPINCP,1)
tail(dtest$PINCP,1)
log(PINCP, base=10)
log(dtest$PINCP, base=10)
log(tail(dtest$PINCP,1), base=10)
log(tail(dtest$PINCP,1), base=10)
inTrain = createDataPartition(y=psub$PINCP,p=0.75, list=FALSE)
dtrain = psub[inTrain,]
dtest = psub[inTrain,]
# Using the training dataset, build model based
# on AGEP, SEX, COW and SCHL independent variables
model = lm(PINCP ~ AGEP + SEX + COW + SCHL,data=dtrain)
# Predict
dtest$predLogPINCP = predict(model,newdata=dtest)
# To discover the accuracy, we plot log income (PINCP) as a function of predicted
# log income (predLogPINCP)
ggplot(data=dtest,aes(x=predLogPINCP,y=PINCP)) +
geom_point(alpha=0.2,color="black") +
geom_smooth(aes(x=predLogPINCP,
y=log(PINCP,base=10)),color="black") +
geom_line(aes(x=log(PINCP,base=10),
y=log(PINCP,base=10)),color="blue",linetype=2) +
scale_x_continuous("Predicted Income Value" +
scale_y_continuous("Actual Income")
# To discover the accuracy, we plot log income (PINCP) as a function of predicted
# log income (predLogPINCP)
ggplot(data=dtest,aes(x=predLogPINCP,y=PINCP) +
geom_point(alpha=0.2,color="black") +
geom_smooth(aes(x=predLogPINCP,
y=log(PINCP,base=10)),color="black") +
geom_line(aes(x=log(PINCP,base=10),
y=log(PINCP,base=10)),color="blue",linetype=2) +
scale_x_continuous("Predicted Income Value" +
scale_y_continuous("Actual Income")
# To discover the accuracy, we plot log income (PINCP) as a function of predicted
# log income (predLogPINCP)
ggplot(data=dtest,aes(x=predLogPINCP,y=log(PINCP,base=10)) +
geom_point(alpha=0.2,color="black") +
geom_smooth(aes(x=predLogPINCP,
y=log(PINCP,base=10)),color="black") +
geom_line(aes(x=log(PINCP,base=10),
y=log(PINCP,base=10)),color="blue",linetype=2) +
scale_x_continuous("Predicted Income Value") +
scale_y_continuous("Actual Income")
# To discover the accuracy, we plot log income (PINCP) as a function of predicted
# log income (predLogPINCP)
ggplot(data=dtest,aes(x=predLogPINCP,y=log(PINCP,base=10))) +
geom_point(alpha=0.2,color="black") +
geom_smooth(aes(x=predLogPINCP,
y=log(PINCP,base=10)),color="black") +
geom_line(aes(x=log(PINCP,base=10),
y=log(PINCP,base=10)),color="blue",linetype=2) +
scale_x_continuous("Predicted Income Value") +
scale_y_continuous("Actual Income")
inTrain = createDataPartition(y=psub$PINCP,p=0.75, list=FALSE)
dtrain = psub[inTrain,]
dtest = psub[inTrain,]
# Using the training dataset, build model based
# on AGEP, SEX, COW and SCHL independent variables
model = lm(log(PINCP,base=10) ~ AGEP + SEX + COW + SCHL,data=dtrain)
# Predict
dtest$predLogPINCP = predict(model,newdata=dtest)
# To discover the accuracy, we plot log income (PINCP) as a function of predicted
# log income (predLogPINCP)
ggplot(data=dtest,aes(x=predLogPINCP,y=log(PINCP,base=10))) +
geom_point(alpha=0.2,color="black") +
geom_smooth(aes(x=predLogPINCP,
y=log(PINCP,base=10)),color="black") +
geom_line(aes(x=log(PINCP,base=10),
y=log(PINCP,base=10)),color="blue",linetype=2) +
scale_x_continuous("Predicted Income Value") +
scale_y_continuous("Actual Income")
# To discover the accuracy, we plot log income (PINCP) as a function of predicted
# log income (predLogPINCP)
ggplot(data=dtest,aes(x=predLogPINCP,y=log(PINCP,base=10))) +
geom_point(alpha=0.2,color="black") +
geom_smooth(aes(x=predLogPINCP,
y=log(PINCP,base=10)),color="black") +
geom_line(aes(x=log(PINCP,base=10),
y=log(PINCP,base=10)),color="blue",linetype=2) +
scale_x_continuous("Predicted Income", limits=c(4,5)) +
scale_y_continuous("Actual Income", limits=c(3.5,5.5))
head(dtest[,c("AGEP","SEX","COW","SCHL","PINCP","predLogPINCP")])
dtest$predLogPINCP = 10^dtest$predLogPINCP
head(dtest[,c("AGEP","SEX","COW","SCHL","PINCP","predLogPINCP")])
inTrain = createDataPartition(y=psub$PINCP,p=0.75, list=FALSE)
dtrain = psub[inTrain,]
dtest1 = psub[inTrain,]
dtest2 = psub[inTrain,]
# Using the training dataset, build model based
# on AGEP, SEX, COW and SCHL independent variables
model = lm(log(PINCP,base=10) ~ AGEP + SEX + COW + SCHL,data=dtrain)
# Predict
dtest1$predLogPINCP = predict(model,newdata=dtest)
head(dtest[,c("AGEP","SEX","COW","SCHL","PINCP","predLogPINCP")])
10^dtest[,c("predLogPINCP")]
10^dtest[1,c("predLogPINCP")]
dtest[1,c("predLogPINCP")]
head(dtest[,c("AGEP","SEX","COW","SCHL","PINCP","predLogPINCP")])
exp(44406.47)
log(22900,base=1o)
log(22900,base=10)
inTrain = createDataPartition(y=psub$PINCP,p=0.75, list=FALSE)
dtrain = psub[inTrain,]
dtest1 = psub[inTrain,]
dtest2 = psub[inTrain,]
# Using the training dataset, build model based
# on AGEP, SEX, COW and SCHL independent variables
model = lm(PINCP ~ AGEP + SEX + COW + SCHL,data=dtrain)
# Predict
dtest1$predLogPINCP = predict(model,newdata=dtest)
head(dtest[,c("AGEP","SEX","COW","SCHL","PINCP","predLogPINCP")])
inTrain = createDataPartition(y=psub$PINCP,p=0.75, list=FALSE)
dtrain = psub[inTrain,]
dtest1 = psub[inTrain,]
dtest2 = psub[inTrain,]
# Using the training dataset, build model based
# on AGEP, SEX, COW and SCHL independent variables
model = lm(log(PINCP,base=10) ~ AGEP + SEX + COW + SCHL,data=dtrain)
# Predict
dtest1$predLogPINCP = predict(model,newdata=dtest1)
head(dtest[,c("AGEP","SEX","COW","SCHL","PINCP","predLogPINCP")])
head(dtest1[,c("AGEP","SEX","COW","SCHL","PINCP","predLogPINCP")])
exp(4.643)
10^4.643077
inTrain = createDataPartition(y=psub$PINCP,p=0.75, list=FALSE)
dtrain = psub[inTrain,]
dtest1 = psub[inTrain,]
dtest2 = psub[inTrain,]
# Using the training dataset, build model based
# on AGEP, SEX, COW and SCHL independent variables
model = lm(PINCP ~ AGEP + SEX + COW + SCHL,data=dtrain)
# Predict
dtest1$predLogPINCP = predict(model,newdata=dtest1)
head(dtest1[,c("AGEP","SEX","COW","SCHL","PINCP","predLogPINCP")])
setwd("c:/Work/Analytics-Edge/Data")
# Read the data set
parole <- read.csv("parole.csv")
# Structure and summary
str(parole)
summary(parole)
# Number of violators
table(parole$violator)[2]
str(parole)
set.seed(144)
library(caTools)
split = sample.split(parole$violator, SplitRatio = 0.7)
train = subset(parole, split == TRUE)
test = subset(parole, split == FALSE)
mod = glm(violator ~ ., data=train, family="binomial")
summary(mod)
# Significant predictors
id = which(summary(mod)$coeff[,4] < 0.05)
# Cofficients of the significant predictors
coeff.sig = summary(mod)$coeff[,1][id]
odds = as.numeric(exp(coefficients(mod)[c("(Intercept)")]+coefficients(mod)[c("male")]+coefficients(mod)[c("race")] + coefficients(mod)[c("time.served")]*3 + coefficients(mod)[c("max.sentence")]*12 + coefficients(mod)[c("multiple.offenses")]*0 + coefficients(mod)[c("crime2")]))
p = odds/(1+odds)
source('~/.active-rstudio-document', echo=TRUE)
install.packages("ROCR")
